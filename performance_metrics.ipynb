{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Computing performance metrices for the given data in different scenarios, without using sklearn library</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Imbalanced dataset (number of positive points >> number of negative points)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     y     proba\n",
      "0  1.0  0.637387\n",
      "1  1.0  0.635165\n",
      "2  1.0  0.766586\n",
      "3  1.0  0.724564\n",
      "4  1.0  0.889199\n"
     ]
    }
   ],
   "source": [
    "# reading the csv file\n",
    "data = pd.read_csv('E:/GITHUB REPOS/performance-metrics-from-scratch/datasets/sample1.csv')\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1.0: 10000, 0.0: 100})\n"
     ]
    }
   ],
   "source": [
    "# printing number of positive(denoted by '1') and negative(denoted by '0') data points\n",
    "print(Counter(data['y']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Function to predict class label of the dataset on the basis of threshold value <br> (If threshold value < 0.5, then treat the label as '0' else treat the label as '1') </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, probability, threshold):\n",
    "  # empty list to store predicted class labels\n",
    "  y_hat = []\n",
    "\n",
    "  for prob_value in data[probability]:\n",
    "\n",
    "     if prob_value < threshold:\n",
    "       y_hat.append(0)\n",
    "     else:\n",
    "       y_hat.append(1)\n",
    "\n",
    "  return y_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><font color='red'>Confusion Matrix</font></h4>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Function to create confusion matrix (confusion matrix has tp,fp,tn,fn)<br>\n",
    "where tp is true positive, fp is false positive, tn is true negative, fn is false negative</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(data, y, y_hat):\n",
    "  # initialize the values of the confusion matrix to zero\n",
    "  tp = 0\n",
    "  fp = 0\n",
    "  tn = 0\n",
    "  fn = 0\n",
    "\n",
    "  for index,label in enumerate(data['y']):\n",
    "    \n",
    "    # if predicted class value is 1 and actual class value is 1, then its true positive\n",
    "    if(data.y_hat[index]==1) and data.y[index]==1:  \n",
    "      tp += 1\n",
    "\n",
    "    # if predicted class value is 1 and actual class value is 0, then its false positive\n",
    "    if(data.y_hat[index]==1) and data.y[index]==0:  \n",
    "      fp += 1\n",
    "\n",
    "    # if predicted class value is 0 and actual class value is 0, then its true negative\n",
    "    if(data.y_hat[index]==0) and data.y[index]==0:  \n",
    "      tn += 1\n",
    "\n",
    "    # if predicted class value is 0 and actual class value is 1, then its false negative\n",
    "    if(data.y_hat[index]==0) and data.y[index]==1:  \n",
    "      fn += 1\n",
    "\n",
    "  return {'tp':tp, 'fp':fp, 'tn':tn, 'fn':fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     y     proba  y_hat\n",
      "0  1.0  0.637387      1\n",
      "1  1.0  0.635165      1\n",
      "2  1.0  0.766586      1\n",
      "3  1.0  0.724564      1\n",
      "4  1.0  0.889199      1\n",
      "\n",
      "The Confusion Matrix is -  {'tp': 10000, 'fp': 100, 'tn': 0, 'fn': 0}\n"
     ]
    }
   ],
   "source": [
    "# threshold set to 0.5 (changes according the problem/business need)\n",
    "threshold = 0.5\n",
    "\n",
    "# calling the predict function and adding a y_hat column to the data\n",
    "data['y_hat'] = predict(data, 'proba', threshold)\n",
    "\n",
    "# updated dataset with y_hat values as well\n",
    "print(data[:5])\n",
    "\n",
    "# calling the confusion matrx function\n",
    "conf_matrix = confusion_matrix(data, 'y', 'y_hat')  \n",
    "\n",
    "# printing the confusion matrix\n",
    "print('\\nThe Confusion Matrix is - ', conf_matrix) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><font color='red'>F1 Score</font></h4>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>For F1 score calculations, first we need to compute precision and recall values</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is -  0.9950248756218906\n"
     ]
    }
   ],
   "source": [
    "precision = conf_matrix['tp']/(conf_matrix['tp'] + conf_matrix['fp'])\n",
    "recall = conf_matrix['tp']/(conf_matrix['tp'] + conf_matrix['fn'])\n",
    "\n",
    "# F1 score formula\n",
    "F1_score = 2*precision*recall/(precision+recall)\n",
    "print('F1 score is - ', F1_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><font color='red'>Accuracy</font></h4>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Accuracy in terms of confusion matrix values is =><br>Total number of correct predictions(positive and negative)/Total number of data points in the dataset</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is -  0.9900990099009901\n"
     ]
    }
   ],
   "source": [
    "accuracy = (conf_matrix['tp']+conf_matrix['tn'])/data.shape[0]\n",
    "print('The accuracy is - ', accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><font color='red'>AUC Score</font></h4>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Function to calculate AUC Score</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_score(data):\n",
    "  # empty lists to store true positive rate values and false positive rate values\n",
    "  tpr, fpr = [], []\n",
    "  \n",
    "  for i in tqdm(data['proba']):\n",
    "\n",
    "    data['y_hat'] = predict(data, 'proba', i)\n",
    "    conf_matrix = confusion_matrix(data, 'y', 'y_hat') \n",
    "\n",
    "    # computing tpr and fpr\n",
    "    tpr.append(conf_matrix['tp']/(conf_matrix['tp']+conf_matrix['fn']))\n",
    "    fpr.append(conf_matrix['fp']/(conf_matrix['tn']+conf_matrix['fp']))\n",
    "\n",
    "    data.drop(columns=['y_hat'])\n",
    "\n",
    "  # calculates the AUC by using trapezium rule\n",
    "  return np.trapz(tpr, fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting the given probability values in descending order\n",
    "data = data.sort_values(by='proba', ascending=False)\n",
    "data = data.drop(columns=['y_hat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10100/10100 [1:49:44<00:00,  1.53it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC Score is -  0.48829900000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# calling the auc function\n",
    "AUCscore = auc_score(data)\n",
    "\n",
    "# printing the AUC score\n",
    "print('The AUC Score is - ', AUCscore)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Imbalanced dataset (number of positive points << number of negative points)</h3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>There is no need to write the predict() and confusion_matrix() functions again. \n",
    "We will just replace the data2 in the place of data.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     y     proba\n",
      "0  0.0  0.281035\n",
      "1  0.0  0.465152\n",
      "2  0.0  0.352793\n",
      "3  0.0  0.157818\n",
      "4  0.0  0.276648\n"
     ]
    }
   ],
   "source": [
    "# reading the csv file\n",
    "data2 = pd.read_csv('E:/GITHUB REPOS/performance-metrics-from-scratch/datasets/sample2.csv')\n",
    "print(data2.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0.0: 10000, 1.0: 100})\n"
     ]
    }
   ],
   "source": [
    "# printing number of positive(denoted by '1') and negative(denoted by '0') data points\n",
    "print(Counter(data2['y']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><font color='red'>Confusion Matrix</font></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is -  {'tp': 55, 'fp': 239, 'tn': 9761, 'fn': 45}\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "data2['y_hat'] = predict(data2, 'proba', threshold)\n",
    "conf_matrix_2 = confusion_matrix(data2, 'y', 'y_hat')\n",
    "\n",
    "# printing confusion matrix of data2\n",
    "print('The confusion matrix is - ', conf_matrix_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><font color='red'>F1 Score</font></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is -  0.2791878172588833\n"
     ]
    }
   ],
   "source": [
    "precision_2 = conf_matrix_2['tp']/(conf_matrix_2['tp'] + conf_matrix_2['fp'])\n",
    "recall_2 = conf_matrix_2['tp']/(conf_matrix_2['tp'] + conf_matrix_2['fn'])\n",
    "\n",
    "F1_score_2 = 2*precision_2*recall_2/(precision_2+recall_2)\n",
    "print('F1 score is - ', F1_score_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><font color='red'>Accuracy</font></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is -  0.9718811881188119\n"
     ]
    }
   ],
   "source": [
    "accuracy_2 = (conf_matrix_2['tp']+conf_matrix_2['tn'])/data2.shape[0]\n",
    "\n",
    "print('The accuracy is - ', accuracy_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><font color='red'>AUC Score</font></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10100/10100 [1:55:29<00:00,  1.46it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC Score is -  0.9377570000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data2 = data2.sort_values(by='proba', ascending=False)\n",
    "data2 = data2.drop(columns=['y_hat'])\n",
    "\n",
    "AUCscore_2 = auc_score(data2)\n",
    "\n",
    "print('The AUC Score is - ', AUCscore_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Calculating MSE, MAPE and R<sup>2</sup> error for a regression problem<br>MSE -> Mean Squared Error<br>MAPE -> Mean Absolute Percentage Error<br>R<sup>2</sup>-> R Squared Error</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       y   pred\n",
      "0  101.0  100.0\n",
      "1  120.0  100.0\n",
      "2  131.0  113.0\n",
      "3  164.0  125.0\n",
      "4  154.0  152.0\n"
     ]
    }
   ],
   "source": [
    "# reading the csv file\n",
    "data3 = pd.read_csv('E:/GITHUB REPOS/performance-metrics-from-scratch/datasets/sample3.csv')\n",
    "print(data3.head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Defining a function to calculate error between the actual label (y) and predicted label (pred)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(data3, y, pred):\n",
    "  error_value = []\n",
    "\n",
    "  for idx, (y_value, pred_value) in enumerate(zip(data3[y], data3[pred])):\n",
    "    error_value.append(y_value - pred_value)\n",
    "  \n",
    "  return error_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the error function and assigning it to newly created column of the dataset which is 'error' column\n",
    "data3['error'] = error(data3, 'y', 'pred')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><font color='red'>Mean Squared Error</font></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining function to calculate mean square error\n",
    "# using already computed error values as an argument in the function so that we dont need to calculate eᵢ again\n",
    "\n",
    "def mean_square_error(data3, error_col):\n",
    "  # initializing square error to zero\n",
    "  square_error = 0  \n",
    "  \n",
    "  for error_value in data3[error_col]:\n",
    "    # incrementing squared error\n",
    "    square_error += (error_value*error_value)\n",
    "\n",
    "  # mean calculated of squared error and returning MSE\n",
    "  return square_error/len(data3[error_col])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Squared Error is -  177.16569974554707\n"
     ]
    }
   ],
   "source": [
    "MSE = mean_square_error(data3, 'error')\n",
    "\n",
    "# printing mean squared error\n",
    "print('The Mean Squared Error is - ', MSE)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><font color='red'>Mean Absolute Percentage Error</font></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining function to calculate absolute error\n",
    "def absolute_error(data3, error_col):\n",
    "  absolute_error_value = []\n",
    "\n",
    "  for error_value in data3[error_col]:\n",
    "    # storing the absolute values of eᵢ in list created\n",
    "    absolute_error_value.append(abs(error_value))\n",
    "  \n",
    "  return absolute_error_value\n",
    "\n",
    "\n",
    "# defining a function to calculate MAPE(Mean Absolute Percentage Error)\n",
    "def mape(data3, abs_error, y):\n",
    "  mape_value = sum(data3[abs_error])/sum(data3[y]) \n",
    "  return mape_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the absolute_error_function and assigning it to newly created column of the dataset which is 'absolute_error' column\n",
    "data3['absolute_error'] = absolute_error(data3, 'error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Absolute Percentage Error is -  0.1291202994009687\n"
     ]
    }
   ],
   "source": [
    "MAPE = mape(data3, 'absolute_error', 'y')\n",
    "\n",
    "# printing mean absolute percentage error\n",
    "print('The Mean Absolute Percentage Error is - ', MAPE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><font color='red'>R Squared Error</font></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to calculate SSᵣₑₛ(residual sum of squares)\n",
    "def residual_ss(data3, error_col):\n",
    "  \n",
    "  # initializing the residual error to zero\n",
    "  residual = 0\n",
    "\n",
    "  for error_value in data3[error_col]:\n",
    "    # incrementing the residual error\n",
    "    residual += (error_value*error_value)  \n",
    "  \n",
    "  return residual\n",
    "\n",
    "\n",
    "# defining a function to calculate SSₜₒₜₐₗ(total sum of squares)\n",
    "def total_ss(data3, y_col): \n",
    "  \n",
    "  # initializing the total error to zero\n",
    "  total = 0  \n",
    "  # mean calculation of actual values\n",
    "  mean_y = data3['y'].mean()  \n",
    "  \n",
    "  for actual_value in data3[y_col]:\n",
    "    # incrementing the total error\n",
    "    total += (actual_value-mean_y)*(actual_value-mean_y)  \n",
    "  \n",
    "  return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² error is -  0.9563582786990964\n"
     ]
    }
   ],
   "source": [
    "SS_residual = residual_ss(data3, 'error')\n",
    "SS_total = total_ss(data3, 'y')\n",
    "\n",
    "# formula of R² error\n",
    "R_squared_error = 1 - (SS_residual/SS_total)  \n",
    "\n",
    "# printing the R² error\n",
    "print('R² error is - ', R_squared_error)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "679401eb40aab2456cb85207dc540ace24e71c0bbf92c8a669e0a910a338b22d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
